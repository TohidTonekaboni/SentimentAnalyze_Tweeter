{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3cdfcf8",
   "metadata": {},
   "source": [
    "# 1 . Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8522b00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import spacy\n",
    "import re\n",
    "import warnings\n",
    "import random\n",
    "import seaborn as sns\n",
    "import string\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b08beba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import RegexpTokenizer, TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter, defaultdict\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d2e900",
   "metadata": {},
   "source": [
    "#### First I load the datset and show 10 random samples of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dd0a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/twitter_training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5770e05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828141c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The shape of the dataset is : {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f19bfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The columns are: {df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8849ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The dtypes of the datset : \\n\\n{df.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21366082",
   "metadata": {},
   "source": [
    "#### Quick review of basic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dfe0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb2aa0",
   "metadata": {},
   "source": [
    "# 2. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a016c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_details(dataset):\n",
    "    missed_values = dataset.isnull().sum()\n",
    "    missed_values_percent = dataset.isnull().sum()/len(dataset)\n",
    "    duplicated_values = dataset.duplicated().sum()\n",
    "    duplicated_values_percent = (dataset.duplicated().sum()) / len(dataset)\n",
    "    info_frame = pd.DataFrame({'Missed_Values' : missed_values , \n",
    "                              'Missed_Values %' :missed_values_percent,\n",
    "                              'Duplicated values' :duplicated_values,\n",
    "                              'Duplicated values %':duplicated_values_percent})\n",
    "    return info_frame.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a40fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_details(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2db84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "show_details(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07509fd",
   "metadata": {},
   "source": [
    "#### Change name of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6c431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'2401':'Index','Borderlands':'Land','Positive':'Mode'\n",
    "                   ,\"im getting on borderlands and i will murder you all ,\": 'Text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bd4649",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dad2068",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The number of unique lands : {len(df.Land.unique())}')\n",
    "print('**' * 40)\n",
    "df.Land.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8684d8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "lands = df.Land.value_counts()\n",
    "lands.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113cccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(10,6))\n",
    "bar = sns.barplot(x=lands.values[:10], y=lands.index[:10], palette='rocket')\n",
    "bar.bar_label(bar.containers[0])\n",
    "plt.title('Top 10 Lands')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Land')\n",
    "plt.xlim(0 , 2500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c8f338",
   "metadata": {},
   "source": [
    "#### columns 'Mode' : Positive, Negative, Neutral, Irrelevent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f32327",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The unique values of Mode : {len(df.Mode.unique())}')\n",
    "print('**' * 20)\n",
    "print(df.Mode.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571d3631",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = df.Mode.value_counts()\n",
    "mode.to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89921f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.pie(x=mode.values, labels=mode.keys(), autopct=\"%1.1f%%\", textprops={\"fontsize\":10,\"fontweight\":\"black\"},\n",
    "       colors=sns.color_palette(\"rocket\"))\n",
    "plt.title('Mode Distribution') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f59dcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.Mode , df.Land).T.style.background_gradient( subset=['Negative'],cmap='Reds')\\\n",
    ".background_gradient(subset=['Positive'] , cmap='Greens')\\\n",
    ".background_gradient(subset=['Irrelevant'] , cmap='BuGn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668ef903",
   "metadata": {},
   "source": [
    "Using re library we can replace a number of grammatically problems, verbal expressions,. ..\n",
    "However I can deal with various emojis which used from people to show off their emotion about the post\n",
    "emojis can play important role in determining the class in the views\n",
    "I have replaced more than 1 puctuations to just 1\n",
    "Finally all texts become lower mode and instead more than 1 space be just 1 space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd36a7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_emoji(tx):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub('r',tx)\n",
    "\n",
    "def text_cleaner(tx):\n",
    "    text = re.sub(r\"won\\'t\", \"would not\", tx)\n",
    "    text = re.sub(r\"im\", \"i am\", tx)\n",
    "    text = re.sub(r\"Im\", \"I am\", tx)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"don\\'t\", \"do not\", text)\n",
    "    text = re.sub(r\"shouldn\\'t\", \"should not\", text)\n",
    "    text = re.sub(r\"needn\\'t\", \"need not\", text)\n",
    "    text = re.sub(r\"hasn\\'t\", \"has not\", text)\n",
    "    text = re.sub(r\"haven\\'t\", \"have not\", text)\n",
    "    text = re.sub(r\"weren\\'t\", \"were not\", text)\n",
    "    text = re.sub(r\"mightn\\'t\", \"might not\", text)\n",
    "    text = re.sub(r\"didn\\'t\", \"did not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\!\\?\\.\\@]',' ' , text)\n",
    "    text = re.sub(r'[!]+' , '!' , text)\n",
    "    text = re.sub(r'[?]+' , '?' , text)\n",
    "    text = re.sub(r'[.]+' , '.' , text)\n",
    "    text = re.sub(r'[@]+' , '@' , text)\n",
    "    text = re.sub(r'unk' , ' ' , text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[ ]+' , ' ' , text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f617fd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(99)\n",
    "test_text =text_cleaner( random.choice(df['Text']))\n",
    "test_text = clean_emoji(test_text)\n",
    "test_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd86100c",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging: POS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c60f89f",
   "metadata": {},
   "source": [
    "The part of speech indicates how the word functions inm meaning as well as gramatically within the sentence.\n",
    "There are 8 parts in english (noun, pronoun, verb, adjective, adverb, preposition, conjunction, interjection)\n",
    "Understanding parts of speech is essential for determining the correct definition of a word when using the dictionary. \n",
    "\n",
    "python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d923de93",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a269c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(test_text)\n",
    "for token in doc :\n",
    "    print(f'{token} => {token.pos_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126b4c9f",
   "metadata": {},
   "source": [
    "### Named Entity Recognition :NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225c5381",
   "metadata": {},
   "source": [
    "NER identifies , categorizes and extract the most important pieces of information from unstrucrured text without requiring time-consuming human analysis. It's particularly useful for quickly extracting key information from large amounts of data because it automates the extraction process.Furthermore , Named entities are specific terms that represent real-world objects, such as people, organizations, locations, and dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a44b3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(test_text)\n",
    "for chunk in doc.ents:\n",
    "    print(f'{chunk} => {chunk.label_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f6e842",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d45800",
   "metadata": {},
   "source": [
    "Converting a text to a smaller pieces for having better undrestanding or the process of grouping similar words together based on the nature of the word. Noun Groups, Verbs, verb groups, etc. \n",
    "NP stands for : Noun Chunks\n",
    "VP : Verp Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa46dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(test_text)\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(f'{chunk} => {chunk.label_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccd569f",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a3e616",
   "metadata": {},
   "source": [
    "Tokenization is breaking text into smaller parts for easier machine analysis, helping machines understand human language. There are various types of tokenizations such as : RegexpTokenizaton , TweetTokenization and etc...\n",
    "which each one has different method for breaking a text into tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37503a74",
   "metadata": {},
   "source": [
    "Here we are using Regexp tokenization which splits a string into substrings using a regular expression. base on space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e37568",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenizer = RegexpTokenizer(r'\\w+')\n",
    "test_text_tokenized = Tokenizer.tokenize(test_text)\n",
    "test_text_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b859d67f",
   "metadata": {},
   "source": [
    "### Count Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8924c3a",
   "metadata": {},
   "source": [
    "Count Vectorizer is used to convert documents, text into vectors of term or token counts, it involves counting the number of occurences of words appears in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52172e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "words  = ['ghost','of','tsushima','now','graphically','is','best','open','world','red','dead','redemption','2','one','second','ahead']\n",
    "counter_vectorizer = CountVectorizer()\n",
    "transform = counter_vectorizer.fit_transform([test_text]).toarray()\n",
    "sns.heatmap(transform, annot=True, xticklabels=words,cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417d8e42",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7632204",
   "metadata": {},
   "source": [
    "Actually , TF-IDF is composed of two parts.Firtsly , Tf which stands for Term Frequancy is how many times a word appears in a document.(counting the number of words and divide it to the number of all words in the sentence)\n",
    "IDF : which stands for Inverse Document Frequancy , is how common a word is found in a corpus or how uncommon a word is found in a corpus.(measure of how important a term is across all documents in the corpus)\n",
    "Result is actually a number between 0 and 1 , It is calculated by taking the logarithm of the total number of documents in the corpus divided by number the of documents in which the term appears\n",
    "However , it can not be a perfect manner to show just a single example to describe TF-IDF , but I will show it for better understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0118e0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "words  = ['ghost','of','tsushima','now','graphically','is','best','open','world','red','dead','redemption','2','one','second','ahead']\n",
    "TF_IDF = TfidfVectorizer()\n",
    "transform = TF_IDF.fit_transform([test_text]).toarray()\n",
    "sns.heatmap(transform, annot=True,xticklabels=words, \n",
    "        cbar=False)\n",
    "transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a858e99d",
   "metadata": {},
   "source": [
    "### N- grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c357b9",
   "metadata": {},
   "source": [
    "N-gram means a sequence of N words.a collection of n successive items in a text document that may include words, numbers, symbols,and punctuation.If 2 , it is based on just the word and the next word after that.3 grams , itself and with 2 words after that and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad10106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams(text, n):\n",
    "\n",
    "    return [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "\n",
    "cleaned = test_text_tokenized\n",
    "\n",
    "n_grams(cleaned, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95220f0",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8332fad2",
   "metadata": {},
   "source": [
    "Stop words are words which are very common in a language. In many projects , they are deleted because they can not affect and they can easily increase the volume of texts without any assistance.\n",
    "Actually , there is a package which contains all stopwords(in NLTK library) in English which are commonly used for NLP projects. First we have to determine our language and download the words for that language.\n",
    "As you can see , there are 179 stopwords in English and I have shown 20 of them ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106a3bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "print(f'There are {len(stopwords_list) } stop words')\n",
    "print('**' * 20 , '\\n20 of them are as follows:\\n')\n",
    "for inx , value in enumerate(stopwords_list[:20]):\n",
    "    print(f'{inx+1}:{value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23693121",
   "metadata": {},
   "source": [
    "### Punctuation count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3236d22b",
   "metadata": {},
   "source": [
    "Now I want to show what punctuations are mostly used in each Mode of views.First, I create a function to read words of each sample for each\n",
    "Mode and then using defaultdict to count each stop words which exists in the whole samples.Using sorted to adjust them with highest one to lowest\n",
    "(for 10 most commons , I have used Bar chart to depict)\n",
    "You can see that , (the , to , and , a) are come the same amount in each Mode with same distribution respectively ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25c1121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_corpus(kind):\n",
    "    corpus = []\n",
    "    for text in df.loc[df['Mode']==kind]['Text'].str.split():\n",
    "        for word in text:\n",
    "            corpus.append(word)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b5a431",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3371a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "stop = stopwords.words('english')\n",
    "sentiments = list(df.Mode.unique())\n",
    "for inx , value in enumerate(sentiments):\n",
    "    corpus = make_corpus(value)\n",
    "    dic = defaultdict(int)\n",
    "    for word in corpus:\n",
    "        if word in stop:\n",
    "            dic[word] += 1\n",
    "            \n",
    "    top = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    x, y = zip(*top)\n",
    "    \n",
    "    plt.title(f'{value} ')\n",
    "    plt.bar(x , y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf580c7f",
   "metadata": {},
   "source": [
    "###### I just do the previous techniques on the dataset to prepare them for the next steps.clear them with text_clearer , and tokenize them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d0824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].apply(lambda x: text_cleaner(x))\n",
    "df['Text'] = df['Text'].apply(lambda x: Tokenizer.tokenize(x))\n",
    "df['Text'].to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2371a04e",
   "metadata": {},
   "source": [
    "#### Lemmatization and Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47477072",
   "metadata": {},
   "source": [
    "Lemmatization is a text pre-processing technique used in natural language processing (NLP) models to break a word down to its root meaning to\n",
    "identify similarities. For example, a lemmatization algorithm would reduce the word better to its root word, or lemme, good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1945a1",
   "metadata": {},
   "source": [
    "Stemming is a natural language processing technique that is used to reduce words to their base form, also known as the root form.The word\n",
    "after stemming is called stem of that word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5375b663",
   "metadata": {},
   "source": [
    "The most important different between Lemmatization and stemming is that , Lemmatization is more accurate and it brings a word to the\n",
    "language root of that word and stemming can be anything which means for computer and is not readable for humans(sometimes not readable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264375a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(test_text)\n",
    "for token in doc :\n",
    "    print(f'{token} => {token.lemma_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc390d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stemmer = PorterStemmer()\n",
    "def stopwords_cleaner(text):\n",
    "#     word = [lemmatizer.lemmatize(letter) for letter in text if letter not in stopwords_list]\n",
    "    word = [Stemmer.stem(letter) for letter in text if letter not in stopwords_list]\n",
    "    peasting = ' '.join(word)\n",
    "    return peasting\n",
    "df['Text'] = df['Text'].apply(lambda x : stopwords_cleaner(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8066a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'][:10].to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c553ec",
   "metadata": {},
   "source": [
    "word Cloud is just a data visualization technique used for representing text data in which the size of each word indicates its frequency or\n",
    "importance.Now , I will show the most important words(common ones) exclude stopwords for each Mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a8a591",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "positive_reviews = df[df['Mode'] == 'Positive']['Text']\n",
    "pos = ' '.join(map(str, positive_reviews))\n",
    "pos_wordcloud = WordCloud(width=1500, height=800,\n",
    "                          background_color='black',\n",
    "                         stopwords=stopwords_list,\n",
    "                          min_font_size=15).generate(pos)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(pos_wordcloud)\n",
    "plt.title('Word Cloud for Positive Reviews')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f5d77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "positive_reviews = df[df['Mode'] == 'Negative']['Text']\n",
    "neg = ' '.join(map(str, positive_reviews))\n",
    "pos_wordcloud = WordCloud(width=1500, height=800,\n",
    "                          background_color='black',\n",
    "                         stopwords=stopwords_list,\n",
    "                          min_font_size=15).generate(neg)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(pos_wordcloud)\n",
    "plt.title('Word Cloud for Negative Reviews')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1ea47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "positive_reviews = df[df['Mode'] == 'Neutral']['Text']\n",
    "Neutral = ' '.join(map(str, positive_reviews))\n",
    "pos_wordcloud = WordCloud(width=1500, height=800,\n",
    "                          background_color='black',\n",
    "                         stopwords=stopwords_list,\n",
    "                          min_font_size=15).generate(Neutral)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(pos_wordcloud)\n",
    "plt.title('Word Cloud for Neutral Reviews')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677d06eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "positive_reviews = df[df['Mode'] == 'Irrelevant']['Text']\n",
    "Irrelevant  = ' '.join(map(str, positive_reviews))\n",
    "pos_wordcloud = WordCloud(width=1500, height=800,\n",
    "                          background_color='black',\n",
    "                         stopwords=stopwords_list,\n",
    "                          min_font_size=15).generate(Irrelevant )\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(pos_wordcloud)\n",
    "plt.title('Word Cloud for Irrelevant Reviews')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb7ce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_text = [len(tx) for tx in df['Text'].to_list()]\n",
    "print(f'Max Length : {np.max(len_text)}')\n",
    "print(f'Min Length : {np.min(len_text)}')\n",
    "print(f'Mean Length : {round(np.mean(len_text),2)}')\n",
    "print(f'Std Length : {round(np.std(len_text),2)}')\n",
    "print(f'Mew + 2sigma : {round(np.mean(len_text)+ 2 *np.std(len_text),2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384244de",
   "metadata": {},
   "source": [
    "##### After removing all stop words we want to see what words are most common for each mode in our corpus.Like wordcloud but in a barchart to have better insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5dd441",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for inx , value in enumerate(sentiments):\n",
    "    \n",
    "    counter = Counter(make_corpus(value))\n",
    "    most_common = counter.most_common()\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for word, count in most_common[:40]:\n",
    "         if word not in stop:\n",
    "            x.append(word)\n",
    "            y.append(count)\n",
    "    sns.barplot(x=y, y=x, orient='h')\n",
    "    plt.title(f'{value} most used words')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3235a670",
   "metadata": {},
   "source": [
    "we are going to create a column namely sentiments and instead of Positive , Negative we put 1 and 0 , instead of Neutral and\n",
    "Irrelevant we put 2. It means that underestanding Positive and Negative reviews is much more important for use rather that neutral and\n",
    "Irrelevant ones.(Just for decreasing the calsses and increasing the accuracy for finding Positive and Negative ones) however , we are able to\n",
    "classify each one of them without mixing , but of curse the accuracy will decrease noticeably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe9a035",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiments'] = df['Mode'].replace({'Positive' : 1 ,  'Negative' : 0 ,'Neutral':2 , 'Irrelevant' : 2 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff595562",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7568f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self,text,sentiment):\n",
    "        self.text = text\n",
    "        self.sentiment = sentiment\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    def __getitem__(self,item):\n",
    "        text = self.text[item,:]\n",
    "        target = self.sentiment[item]\n",
    "        return {\n",
    "            \"text\": torch.tensor(text,dtype = torch.long),\n",
    "            \"target\": torch.tensor(target,dtype = torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831cb727",
   "metadata": {},
   "source": [
    "We require helper functions to help us reading word vectors(we have a vord vector to give our model to understand the distant meaning of each words\n",
    "and by that we can improve our model accuracy. There are many word vectors whcih I will use glove.6B.300d 300 is the dimention.you can use less\n",
    "ones. More is better understanding but computationally higher.)and them another function to help us create embedding matrix for our corpus words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e02a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname):\n",
    "    fin = open(fname , encoding=\"utf8\")\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.split()\n",
    "        data[tokens[0]] = np.array([float(value) for value in tokens[1:]])\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f32506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(word_index,embedding_dict):\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_index)+1,300))\n",
    "    for word, i in word_index.items():\n",
    "        if word in embedding_dict:\n",
    "            embedding_matrix[i] = embedding_dict[word]\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35beaaf7",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501a4bbf",
   "metadata": {},
   "source": [
    "Bidirectional LSTM (BiLSTM) is a recurrent neural network used primarily on natural language processing. Unlike standard LSTM, the input flows in both directions, and it’s capable of utilizing information from both sides. It’s also a powerful tool for modeling the sequential dependencies between words and phrases in both directions of the sequence.\n",
    "\n",
    "In summary, BiLSTM adds one more LSTM layer, which reverses the direction of information flow. Briefly, it means that the input sequence flows backward in the additional LSTM layer. Then we combine the outputs from both LSTM layers in several ways, such as average, sum, multiplication, or concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425283ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentimentBiLSTM(nn.Module):\n",
    "#inherited from nn.Module\n",
    "    def __init__(self,embedding_matrix,hidden_dim,output_size):\n",
    "        \n",
    "        #initializing the params by initialization method\n",
    "        \n",
    "        super(sentimentBiLSTM,self).__init__()\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.hidden_dim = hidden_dim\n",
    "        num_words = self.embedding_matrix.shape[0]\n",
    "        embed_dim = self.embedding_matrix.shape[1]\n",
    "        \n",
    "        # creating embedding layer\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=num_words,embedding_dim=embed_dim)\n",
    "        \n",
    "        ## initializes the weights of the embedding layer to the pretrained embeddings in \n",
    "        ## embedding_matrix. It first converts embedding_matrix to a PyTorch tensor and \n",
    "        ## wraps it in an nn.Parameter object, which makes it a learnable parameter of the model.\n",
    "        \n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix,dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(embed_dim,hidden_dim,bidirectional=True,batch_first=True)\n",
    "        \n",
    "        #it is multiplied by 2 becuase it is bi_directional if one-sided it didnt need.\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_size)\n",
    "        \n",
    "    #we need a forward function to model calculate the cost and know how bad the params is .  \n",
    "    # However , it can be written in a line of code but if we want to track it it is easier way.\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out,_ = self.lstm(embeds)\n",
    "        lstm_out = lstm_out[:, -1]\n",
    "        out = self.fc(lstm_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cfb3e0",
   "metadata": {},
   "source": [
    "### Spliting data to train and test => 80% for train and 20% for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d927d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.sentiments.values\n",
    "train_df,test_df = train_test_split(df,test_size = 0.2, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88f247",
   "metadata": {},
   "source": [
    "Max_length as mew + 2sigma = 167\n",
    "Batch size is a number that detremined based on your system 16-32-64...\n",
    "Hidden_dimention for the model will be 64\n",
    "output is the number of classes which we have (len(classes))\n",
    "Also check if Cuda is available we put our system on GPU else CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa7603b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 167\n",
    "BATCH_SIZE = 32\n",
    "hidden_dim = 64\n",
    "output_size = 3\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")   \n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(f'Current device is {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695a3380",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(df.Text.values.tolist())\n",
    "\n",
    "xtrain = tokenizer.texts_to_sequences(train_df.Text.values)\n",
    "xtest = tokenizer.texts_to_sequences(test_df.Text.values)\n",
    "\n",
    "xtrain = tf.keras.preprocessing.sequence.pad_sequences(xtrain,maxlen = MAX_LEN)\n",
    "xtest = tf.keras.preprocessing.sequence.pad_sequences(xtest,maxlen = MAX_LEN)\n",
    "\n",
    "train_dataset = Dataset(text=xtrain,sentiment=train_df.sentiments.values)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=BATCH_SIZE,drop_last=True)\n",
    "\n",
    "valid_dataset = Dataset(text=xtest,sentiment=test_df.sentiments.values)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=BATCH_SIZE,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f28b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check a batch of data \n",
    "one_batch = next(iter(train_loader))\n",
    "one_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9d7f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = load_vectors('Data/glove.6B.300d.txt')\n",
    "embedding_matrix = create_embedding_matrix(tokenizer.word_index,embedding_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ddff10",
   "metadata": {},
   "source": [
    "Now create a object of the model (embedding_matrix, hidden_dim=64, output_size=3) and put the model on device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5d5cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sentimentBiLSTM(embedding_matrix ,  hidden_dim, output_size)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f599d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42) \n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "# schedul_learning = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer , milestones=[6] ,\n",
    "#                                                         gamma=0.055)\n",
    "\n",
    "def acc(pred,label):\n",
    "    pred = pred.argmax(1)\n",
    "    return torch.sum(pred == label.squeeze()).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2accb629",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = 5\n",
    "epochs = 9\n",
    "valid_loss_min = np.Inf\n",
    "# train for some number of epochs\n",
    "epoch_tr_loss,epoch_vl_loss = [],[]\n",
    "epoch_tr_acc,epoch_vl_acc = [],[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # for getting loss and accuracy for train\n",
    "    train_losses = []\n",
    "    train_acc = 0.0\n",
    "\n",
    "    #put model on train mode\n",
    "    model.train()\n",
    "    correct = 0\n",
    "\n",
    "    # initialize hidden state \n",
    "    for data in train_loader:  \n",
    "\n",
    "        #get text and target \n",
    "        inputs = data['text']\n",
    "        labels = data['target']\n",
    "\n",
    "        #put them on GPU and right dtypes\n",
    "        inputs = inputs.to(device,dtype=torch.long)\n",
    "        labels = labels.to(device,dtype=torch.float)\n",
    "\n",
    "         #gradient becomes zero=> avoid accumulating \n",
    "        model.zero_grad()\n",
    "        output = model(inputs)\n",
    "          # calculate the loss and perform backprop\n",
    "        loss = criterion(output, labels.long())\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "        # accuracy\n",
    "        accuracy = acc(output,labels)\n",
    "        train_acc += accuracy\n",
    "        #`clip_grad_norm` helps prevent the exploding gradient problem in LSTMs\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "    # for getting loss and accuracy for valiadtion\n",
    "    val_losses = []\n",
    "    val_acc = 0.0\n",
    "\n",
    "    #put model on evaluation mode\n",
    "    model.eval()\n",
    "    for data in valid_loader:\n",
    "\n",
    "        #get text and target \n",
    "        inputs = data['text']\n",
    "        labels = data['target']\n",
    "\n",
    "        #put them on GPU and right dtypes\n",
    "        inputs = inputs.to(device,dtype=torch.long)\n",
    "        labels = labels.to(device,dtype=torch.float)\n",
    "\n",
    "        #gradient becomes zero=> avoid accumulating \n",
    "        model.zero_grad()\n",
    "        output = model(inputs)\n",
    "\n",
    "        output = model(inputs)\n",
    "        #Loss calculating \n",
    "        val_loss = criterion(output, labels.long())\n",
    "        #append Loss to the above list\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "        # calculating accuracy \n",
    "        accuracy = acc(output,labels)\n",
    "        val_acc += accuracy\n",
    "        epoch_train_loss = np.mean(train_losses)\n",
    "\n",
    "        #using schedule lr if you need\n",
    "#         schedul_learning.step()\n",
    "#         schedul_learning\n",
    "\n",
    "    #appending all accuracy and loss to the above lists and variables\n",
    "    epoch_val_loss = np.mean(val_losses)\n",
    "    epoch_train_acc = train_acc/len(train_loader.dataset)\n",
    "    epoch_val_acc = val_acc/len(valid_loader.dataset)\n",
    "    epoch_tr_loss.append(epoch_train_loss)\n",
    "    epoch_vl_loss.append(epoch_val_loss)\n",
    "    epoch_tr_acc.append(epoch_train_acc)\n",
    "    epoch_vl_acc.append(epoch_val_acc)\n",
    "    print(f'Epoch {epoch+1}') \n",
    "    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n",
    "    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n",
    "    if epoch_val_loss <= valid_loss_min:\n",
    "        #each time that model(params) get better you can save the model(you have to enter a path ou you pc and save with pt file)\n",
    "        # torch.save(model.state_dict(), r'C:\\Users\\payama\\Desktop\\Projects kaggle\\NLP\\vectors features\\BidirectionalLSTM.pt')\n",
    "#         print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n",
    "        print(f'Validation loss decreased ({valid_loss_min} --> {epoch_val_loss})  Saving model ...')\n",
    "        # save model if better result happends\n",
    "        valid_loss_min = epoch_val_loss\n",
    "    print(30 * '==' , '>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860c3a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(range(1,10),epoch_tr_acc , label='train accuracy')\n",
    "plt.scatter(range(1,10),epoch_tr_acc)\n",
    "plt.plot(range(1,10),epoch_vl_acc , label='val accuracy')\n",
    "plt.scatter(range(1,10),epoch_vl_acc)\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f229ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(range(1,10),epoch_tr_loss , label='train loss')\n",
    "plt.scatter(range(1,10),epoch_tr_loss )\n",
    "plt.plot(range(1,10),epoch_vl_loss , label='val loss')\n",
    "plt.scatter(range(1,10),epoch_vl_loss)\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
